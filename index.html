<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenSubstance: A High-Quality Measured Dataset of Multi-View and -Lighting Images and Shapes</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Roboto', 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #ffffff;
            font-size: 16px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        .header {
            text-align: center;
            margin-bottom: 50px;
        }

        .title {
            font-size: 2.6rem;
            font-weight: 300;
            color: #2c3e50;
            margin-bottom: 20px;
            letter-spacing: -0.02em;
            line-height: 1.2;
        }

        .venue {
            font-size: 1.3rem;
            color: #e74c3c;
            font-weight: 500;
            margin-bottom: 25px;
        }

        .authors {
            font-size: 1.1rem;
            color: #555;
            margin-bottom: 15px;
            line-height: 1.5;
        }

        .authors a {
            color: #3498db;
            text-decoration: none;
        }

        .authors a:hover {
            text-decoration: underline;
        }

        .affiliation {
            font-size: 1rem;
            color: #777;
            margin-bottom: 30px;
        }

        .dataset-preview {
            text-align: center;
            margin: 40px 0;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border-radius: 12px;
            padding: 40px;
            color: white;
        }

        .dataset-stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-top: 30px;
        }

        .stat-card {
            background: rgba(255, 255, 255, 0.1);
            padding: 20px;
            border-radius: 8px;
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.2);
        }

        .stat-number {
            font-size: 2rem;
            font-weight: bold;
            display: block;
        }

        .stat-label {
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .paper-image {
            text-align: center;
            margin: 40px 0;
        }

        .paper-image img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
        }

        .links {
            text-align: center;
            margin: 40px 0;
            display: flex;
            justify-content: center;
            flex-wrap: wrap;
            gap: 15px;
        }

        .link-btn {
            display: inline-block;
            padding: 12px 24px;
            margin: 8px 12px;
            background-color: #2c3e50;
            color: white;
            text-decoration: none;
            border-radius: 5px;
            font-weight: 500;
            font-size: 1rem;
            transition: all 0.3s ease;
            border: 2px solid #2c3e50;
        }

        .link-btn:hover {
            background-color: #34495e;
            border-color: #34495e;
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
        }

        .link-btn.disabled {
            background-color: #95a5a6;
            border-color: #95a5a6;
            cursor: not-allowed;
            position: relative;
        }

        .link-btn.disabled:hover {
            background-color: #95a5a6;
            border-color: #95a5a6;
            transform: none;
            box-shadow: none;
        }

        .coming-soon {
            position: absolute;
            top: -8px;
            right: -8px;
            background-color: #f39c12;
            color: white;
            font-size: 0.7rem;
            padding: 2px 6px;
            border-radius: 3px;
            font-weight: 600;
        }

        .abstract {
            background-color: #f8f9fa;
            padding: 30px;
            border-radius: 8px;
            margin: 40px 0;
            border-left: 4px solid #2c3e50;
        }

        .abstract h2 {
            color: #2c3e50;
            margin-bottom: 20px;
            font-size: 1.5rem;
            font-weight: 400;
        }

        .abstract p {
            color: #555;
            line-height: 1.7;
            text-align: justify;
        }

        .contributions {
            background-color: #e8f5e8;
            padding: 30px;
            border-radius: 8px;
            margin: 40px 0;
            border-left: 4px solid #27ae60;
        }

        .contributions h2 {
            color: #27ae60;
            margin-bottom: 20px;
            font-size: 1.5rem;
            font-weight: 400;
        }

        .contributions ul {
            color: #555;
            line-height: 1.7;
            padding-left: 20px;
        }

        .contributions li {
            margin-bottom: 10px;
        }

        .acquisition-setup {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 30px;
            margin: 40px 0;
        }

        .setup-card {
            background: white;
            border-radius: 8px;
            padding: 25px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            border: 1px solid #eee;
        }

        .setup-card h3 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.2rem;
        }

        .setup-card p {
            color: #666;
            line-height: 1.6;
        }

        .citation {
            background-color: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 8px;
            margin: 40px 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            overflow-x: auto;
        }

        .citation pre {
            margin: 0;
            white-space: pre-wrap;
        }

        .footer {
            text-align: center;
            padding: 40px 0;
            color: #7f8c8d;
            border-top: 1px solid #ecf0f1;
            margin-top: 60px;
        }

        @media (max-width: 768px) {
            .container {
                padding: 20px 15px;
            }
            
            .title {
                font-size: 2rem;
            }
            
            .venue {
                font-size: 1.1rem;
            }
            
            .link-btn {
                display: block;
                margin: 10px auto;
                max-width: 200px;
            }
            
            .abstract, .contributions {
                padding: 20px;
            }

            .dataset-stats {
                grid-template-columns: 1fr 1fr;
            }
        }

        .highlight {
            background-color: #fff3cd;
            padding: 15px;
            border-radius: 5px;
            border-left: 4px solid #ffc107;
            margin: 20px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="header">
            <h1 class="title">OpenSubstance: A High-Quality Measured Dataset of Multi-View and -Lighting Images and Shapes</h1>
            <div class="venue">ICCV 2025</div>
            <div class="authors">
                Fan Pei</a>,
                Jinchen Bai</a>,
                <a href="https://sigfeng.com/">Xiang Feng</a>,
                <a href="https://github.com/RupertPaoZ">Zoubin Bi</a>,
                Kun Zhou</a><sup>‚Ä†</sup>,
                <a href="https://svbrdf.github.io/">Hongzhi Wu</a><sup>‚Ä†</sup>
            </div>
            <div class="affiliation">
                State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China<br>
                <sup>‚Ä†</sup>Corresponding authors
            </div>
        </header>



        <div class="links">
            <a href="https://drive.google.com/file/d/1hQWlmYmP-RVrX0EFD_ulyJ1RAr9FxeDd/view?usp=sharing" class="link-btn">üìÑ Paper</a>
            <a href="https://drive.google.com/drive/folders/1WjWgG1agL7x0R7Ty1tEYu8IY3-MOwGWS?usp=drive_link" class="link-btn">üñºÔ∏è Gallery</a>
            <a href="https://drive.google.com/file/d/1wU0SdeU5JeQU9WGJ1D9KlCk6HKMh6IYD/view?usp=drive_link" class="link-btn">üìã Poster</a>
            <div style="position: relative;">
                <a href="#data" class="link-btn disabled">üìä Data</a>
                <span class="coming-soon">Coming Soon</span>
            </div>
        </div>

        <div class="highlight">
            </strong> The dataset release is pending university storage allocation due to its substantial size (almost 20 TB). Download access will be available once infrastructure is in place.
        </div>

        <section class="abstract">
            <h2>Abstract</h2>
            <p>
                We present OpenSubstance, a high-quality measured dataset with 2.4 million high-dynamic-range images of 187 objects with a wide variety in shape and appearance, captured under 270 camera views and 1,637 lighting conditions, including 1,620 one-light-at-a-time, 8 environment, 8 linear and 1 full-on illumination. For each image, the corresponding lighting condition, camera parameters and foreground segmentation mask are provided. High-precision 3D geometry is also acquired for rigid objects. It takes 1 hour on average to capture one object with our custom-built high-performance lightstage and a top-grade commercial 3D scanner. We perform comprehensive quantitative evaluation on state-of-the-art techniques across different tasks, including single- and multi-view photometric stereo, as well as relighting.
            </p>
        </section>



        <div class="acquisition-setup">
            <div class="setup-card">
                <h3>üîß Hardware Setup</h3>
                <p>Custom lightstage with 24,576 high-brightness LEDs, 6 machine vision cameras (5328√ó4608 resolution), and digital turntable for precise multi-view capture.</p>
            </div>
            <div class="setup-card">
                <h3>üí° Lighting Conditions</h3>
                <p>1,620 one-light-at-a-time (OLAT), 8 environment maps, 8 linear lighting, and 1 full-on illumination for comprehensive appearance modeling.</p>
            </div>
            <div class="setup-card">
                <h3>üìê 3D Scanning</h3>
                <p>ZEISS ATOS Q Blue-Light 3D scanner with single-digit micron accuracy for high-precision ground truth geometry acquisition.</p>
            </div>
        </div>



        <footer class="footer">
            <p>¬© 2025 State Key Lab of CAD&CG, Zhejiang University. For questions, contact: {kunzhou,hwu}@acm.org</p>
        </footer>
    </div>

    <script>
        // Smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const targetId = this.getAttribute('href').substring(1);
                const targetElement = document.getElementById(targetId);
                if (targetElement) {
                    targetElement.scrollIntoView({
                        behavior: 'smooth'
                    });
                }
            });
        });

        // Add loading animation
        window.addEventListener('load', function() {
            document.body.style.opacity = '0';
            document.body.style.transition = 'opacity 0.6s ease-in-out';
            setTimeout(() => {
                document.body.style.opacity = '1';
            }, 100);
        });

        // Interactive stats animation
        const observerOptions = {
            threshold: 0.1,
            rootMargin: '0px 0px -50px 0px'
        };

        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.style.transform = 'scale(1.05)';
                    entry.target.style.transition = 'transform 0.3s ease';
                    setTimeout(() => {
                        entry.target.style.transform = 'scale(1)';
                    }, 300);
                }
            });
        }, observerOptions);

        document.querySelectorAll('.stat-card').forEach(card => {
            observer.observe(card);
        });
    </script>
</body>
</html>